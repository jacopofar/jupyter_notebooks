{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add punctuation to a text\n",
    "\n",
    "In this notebook we'll try to build a small neural network to apply punctuation to a text, using Skorch to integrate Pytorch with Scikit-learn and be able to easily compare the performance with other ML tools for this use case.\n",
    "\n",
    "For simplicity, the network will be quite small and fast to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterances: 400000\n",
      "Longest one (chars): 19851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gzip\n",
    "from random import shuffle\n",
    "\n",
    "MAX_UTTERANCES_TO_LOAD = 400_000\n",
    "\n",
    "utterances = []\n",
    "\n",
    "with gzip.open('paisa.raw.utf8.gz', 'rb') as f:\n",
    "    for line in (l.decode() for l in f):\n",
    "        if len(line.strip()) < 15:\n",
    "            continue\n",
    "        if line.startswith('<text') or line.startswith('</text>'):\n",
    "            continue\n",
    "        utterances.append(line.strip())\n",
    "        if len(utterances) >= MAX_UTTERANCES_TO_LOAD:\n",
    "            break\n",
    "\n",
    "shuffle(utterances)\n",
    "\n",
    "# rough number of tokens\n",
    "print('Utterances:', len(utterances))\n",
    "print('Longest one (chars):', max(len(u) for u in utterances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function return tuples of text and punctuation, using `isalpha()` and `isdigit()` to tell them apart and accept multiple characters for the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_punctuation_tuples(text: str):\n",
    "    \"\"\"Return tuples of text and subsequent punctuation from a text.\"\"\"\n",
    "    am_in_token = True\n",
    "    token_start = 0\n",
    "    punctuation_start = 0\n",
    "    \n",
    "    for idx, char in enumerate(text):\n",
    "        if am_in_token:\n",
    "            if char.isalpha() or char.isdigit():\n",
    "                # just a new char for this token\n",
    "                pass\n",
    "            else:\n",
    "                # switch to punctuation\n",
    "                am_in_token = False\n",
    "                punctuation_start = idx\n",
    "        else:\n",
    "            if char.isalpha() or char.isdigit():\n",
    "                # a new token, the punctuation is over\n",
    "                am_in_token = True\n",
    "                yield (\n",
    "                    text[token_start:punctuation_start],\n",
    "                    text[punctuation_start:idx])\n",
    "                token_start = idx\n",
    "            else:\n",
    "                # just a new char for the punctuation\n",
    "                pass\n",
    "    \n",
    "    if am_in_token:\n",
    "        # the last text has an empty token associated\n",
    "        yield (text[token_start:], '')\n",
    "    else:\n",
    "        # the punctuation was the end\n",
    "        yield (text[token_start:punctuation_start], text[punctuation_start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ma', ' '\n",
      "'non', ' '\n",
      "'disperiamo', '.'\n",
      "'Siamo', ' '\n",
      "'incoraggiati', ' '\n",
      "'da', ' '\n",
      "'quello', ' '\n",
      "'che', ' '\n",
      "'abbiamo', ' '\n",
      "'letto', ', '\n",
      "'sentito', ' '\n",
      "'alla', ' '\n",
      "'radio', ', '\n",
      "'o', ' '\n",
      "'visto', ' '\n",
      "'in', ' '\n",
      "'TV', '; '\n",
      "'riguardo', ' '\n",
      "'a', ' '\n",
      "'ciò', ' '\n",
      "'che', ' '\n",
      "'i', ' '\n",
      "'nostri', ' '\n",
      "'fratelli', ' '\n",
      "'e', ' '\n",
      "'sorelle', ' '\n",
      "'del', ' '\n",
      "'Nord', ' '\n",
      "'stanno', ' '\n",
      "'facendo', ' '\n",
      "'per', ' '\n",
      "'determinare', ' '\n",
      "'la', ' '\n",
      "'forma', ' '\n",
      "'che', ' '\n",
      "'dovrà', ' '\n",
      "'prendere', ' '\n",
      "'la', ' '\n",
      "'nuova', ' '\n",
      "'economia', ' '\n",
      "'mondiale', '. '\n",
      "'I', ' '\n",
      "'loro', ' '\n",
      "'modi', ' '\n",
      "'di', ' '\n",
      "'lottare', ' '\n",
      "'sono', ' '\n",
      "'allo', ' '\n",
      "'stesso', ' '\n",
      "'tempo', ' '\n",
      "'così', ' '\n",
      "'simili', ' '\n",
      "'e', ' '\n",
      "'così', ' '\n",
      "'diversi', ' '\n",
      "'dai', ' '\n",
      "'nostri', '. '\n",
      "'Mano', ' '\n",
      "'a', ' '\n",
      "'mano', ' '\n",
      "'che', ' '\n",
      "'la', ' '\n",
      "'nostra', ' '\n",
      "'lotta', ' '\n",
      "'emerge', ', '\n",
      "'impariamo', ' '\n",
      "'meglio', ' '\n",
      "'come', ' '\n",
      "'combattere', ' '\n",
      "'con', ' '\n",
      "'più', ' '\n",
      "'forza', ' '\n",
      "'chi', ' '\n",
      "'ci', ' '\n",
      "'sta', ' '\n",
      "'facendo', ' '\n",
      "'del', ' '\n",
      "'male', '. '\n",
      "'Non', ' '\n",
      "'faremo', ' '\n",
      "'gli', ' '\n",
      "'errori', ' '\n",
      "'del', ' '\n",
      "'passato', ', '\n",
      "'quando', ' '\n",
      "'troppo', ' '\n",
      "'spesso', ' '\n",
      "'abbiamo', ' '\n",
      "'confidato', ' '\n",
      "'in', ' '\n",
      "'leader', ' '\n",
      "'o', ' '\n",
      "'partiti', ' '\n",
      "'o', ' '\n",
      "'nazione', ' '\n",
      "'o', ' '\n",
      "'razze', ', '\n",
      "'perché', ' '\n",
      "'ci', ' '\n",
      "'portassero', ' '\n",
      "'alla', ' '\n",
      "'salvezza', '. '\n",
      "'Ora', ' '\n",
      "'sappiamo', ' '\n",
      "'che', ' '\n",
      "'solo', ' '\n",
      "'la', ' '\n",
      "'libertà', ' '\n",
      "'e', ' '\n",
      "'la', ' '\n",
      "'giustizia', ' '\n",
      "'che', ' '\n",
      "'noi', ' '\n",
      "'tutti', ' '\n",
      "'costruiremo', ' '\n",
      "'assieme', ' '\n",
      "'ha', ' '\n",
      "'la', ' '\n",
      "'forza', ' '\n",
      "'per', ' '\n",
      "'resistere', ' '\n",
      "'alla', ' '\n",
      "'oppressione', '.'\n"
     ]
    }
   ],
   "source": [
    "for w, p in words_punctuation_tuples(utterances[0]):\n",
    "    print(f\"'{w}', '{p}'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 20683575\n",
      "Will use the 40 most common, which are:\n",
      "Symbol \t' '\t appears 16522341 \t times (79.88%)\n",
      "Symbol \t', '\t appears 1113198 \t times (5.38%)\n",
      "Symbol \t'. '\t appears 462556 \t times (2.24%)\n",
      "Symbol \t'’'\t appears 263918 \t times (1.28%)\n",
      "Symbol \t'.'\t appears 242113 \t times (1.17%)\n",
      "Symbol \t'/'\t appears 200281 \t times (0.97%)\n",
      "Symbol \t'''\t appears 189360 \t times (0.92%)\n",
      "Symbol \t' ('\t appears 141903 \t times (0.69%)\n",
      "Symbol \t': '\t appears 137121 \t times (0.66%)\n",
      "Symbol \t'-'\t appears 131778 \t times (0.64%)\n",
      "Symbol \t''\t appears 126714 \t times (0.61%)\n",
      "Symbol \t'_'\t appears 116813 \t times (0.56%)\n",
      "Symbol \t')'\t appears 62700 \t times (0.30%)\n",
      "Symbol \t') '\t appears 50220 \t times (0.24%)\n",
      "Symbol \t' “'\t appears 48138 \t times (0.23%)\n",
      "Symbol \t' /'\t appears 44401 \t times (0.21%)\n",
      "Symbol \t' - '\t appears 43315 \t times (0.21%)\n",
      "Symbol \t'' '\t appears 38263 \t times (0.18%)\n",
      "Symbol \t' \"'\t appears 36246 \t times (0.18%)\n",
      "Symbol \t'(): '\t appears 35438 \t times (0.17%)\n",
      "Symbol \t'’ '\t appears 34328 \t times (0.17%)\n",
      "Symbol \t'; '\t appears 28420 \t times (0.14%)\n",
      "Symbol \t'” '\t appears 27223 \t times (0.13%)\n",
      "Symbol \t'? '\t appears 25451 \t times (0.12%)\n",
      "Symbol \t','\t appears 21601 \t times (0.10%)\n",
      "Symbol \t'), '\t appears 21030 \t times (0.10%)\n",
      "Symbol \t'\" '\t appears 20192 \t times (0.10%)\n",
      "Symbol \t' ''\t appears 13862 \t times (0.07%)\n",
      "Symbol \t' «'\t appears 13613 \t times (0.07%)\n",
      "Symbol \t' – '\t appears 13273 \t times (0.06%)\n",
      "Symbol \t'). '\t appears 11760 \t times (0.06%)\n",
      "Symbol \t'”, '\t appears 11539 \t times (0.06%)\n",
      "Symbol \t'! '\t appears 10853 \t times (0.05%)\n",
      "Symbol \t':'\t appears 10494 \t times (0.05%)\n",
      "Symbol \t'' ('\t appears 8984 \t times (0.04%)\n",
      "Symbol \t'\", '\t appears 8945 \t times (0.04%)\n",
      "Symbol \t']: '\t appears 8932 \t times (0.04%)\n",
      "Symbol \t'() ['\t appears 8841 \t times (0.04%)\n",
      "Symbol \t''@''\t appears 8841 \t times (0.04%)\n",
      "Symbol \t'?'\t appears 8809 \t times (0.04%)\n",
      "---\n",
      "Eventually covering 20323808 out of 20683575 (98.26%)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# possible punctuations used for hot encoding, ignore the others\n",
    "KNOWN_PUNCTUATIONS = 40\n",
    "\n",
    "punctuation = Counter([punct for ut in utterances for _, punct in words_punctuation_tuples(ut)])\n",
    "total_tokens = sum(punctuation.values())\n",
    "print(f'Total tokens: {total_tokens}')\n",
    "print(f'Will use the {KNOWN_PUNCTUATIONS} most common, which are:')\n",
    "\n",
    "total_partial = 0\n",
    "punct_categories = []\n",
    "for punct, count in punctuation.most_common(KNOWN_PUNCTUATIONS):\n",
    "    punct_categories.append(punct)\n",
    "    punct = punct.replace('\\n', '\\\\n')\n",
    "    total_partial += count\n",
    "    print(f\"Symbol \\t'{punct}'\\t appears {count} \\t times\"\n",
    "          f' ({count * 100 / total_tokens:0.2f}%)')\n",
    "print('---')\n",
    "print(f'Eventually covering {total_partial} out of {total_tokens}'\n",
    "      f' ({total_partial * 100 / total_tokens:0.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequencies, uncased\n",
    "words_freq = Counter([word for ut in utterances for word, _ in words_punctuation_tuples(ut)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens for the hot encoding, ignore the others\n",
    "# or if using embeddings, the size of the embedding + casing encoding\n",
    "TOKEN_INPUT_SIZE = 100 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common five words: [('di', 734875), ('e', 514138), ('che', 397898), ('il', 321847), ('la', 321756)]\n",
      "With 102 we cover 8453963 tokens out of 20683575, (40.87% of the total)\n",
      "Original: [['il']\n",
      " ['che']]\n",
      "Reconstructed: [['il']\n",
      " ['che']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "token_categories = [w for w, _ in words_freq.most_common(TOKEN_INPUT_SIZE)]\n",
    "total_covered = sum(c for _, c in words_freq.most_common(TOKEN_INPUT_SIZE))\n",
    "print(f'Most common five words: {words_freq.most_common(5)}')\n",
    "print(f'With {TOKEN_INPUT_SIZE} we cover {total_covered} tokens out of {total_tokens}'\n",
    "      f', ({total_covered * 100 / total_tokens:0.2f}% of the total)')\n",
    "\n",
    "# map them to non-sparse one-hot encoding\n",
    "oh_enc_words = OneHotEncoder(\n",
    "    sparse=False, # use an array\n",
    "    handle_unknown='ignore', # return all 0 in case of unknown string, no error\n",
    "    categories='auto', # enumerate categories later, during training\n",
    "    dtype=np.float32, # use float32 or pytorch will convert to double\n",
    ")\n",
    "\n",
    "oh_enc_words.fit_transform(np.array(token_categories).reshape(-1, 1))\n",
    "\n",
    "# show that it works both ways\n",
    "tokens = np.array(['il', 'che']).reshape(-1, 1)\n",
    "encoded = oh_enc_words.transform(tokens)\n",
    "decoded = oh_enc_words.inverse_transform(encoded)\n",
    "print(f'Original: {tokens}')\n",
    "print(f'Reconstructed: {decoded}')\n",
    "assert np.array_equal(decoded, tokens)\n",
    "\n",
    "# show that it ignores unknown tokens\n",
    "tokens = np.array(['fakewordnotexisting']).reshape(-1, 1)\n",
    "encoded = oh_enc_words.transform(tokens)\n",
    "assert np.all(encoded == 0)\n",
    "\n",
    "del encoded, decoded, tokens\n",
    "\n",
    "# same with punctuation\n",
    "\n",
    "oh_enc_punct = OneHotEncoder(\n",
    "    sparse=False,\n",
    "    handle_unknown='ignore',\n",
    "    categories='auto' # enumerate categories later, during training\n",
    ")\n",
    "oh_enc_punct.fit_transform(np.array(punct_categories).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to encode words this way, but then let's encode using word embeddings. I use my own GloVe data for Italian, cased, available here: https://github.com/jacopofar/glove-tools/releases\n",
    "\n",
    "Still the function based on embeddings is available and can be used by changing the `ENCODER_FUNCTION` variable accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n",
      "[[-8.842800e-02 -3.105260e-01 -2.880390e-01  2.165120e-01  5.492700e-02\n",
      "  -4.553560e-01 -3.298800e-02 -3.642840e-01  3.621540e-01  5.890700e-02\n",
      "  -5.337540e-01 -9.265580e-01 -1.334600e-02  8.907450e-01  2.797360e-01\n",
      "  -1.371800e+00 -3.160470e-01  2.703050e-01  1.323670e-01  5.223730e-01\n",
      "  -1.658090e-01  3.485090e-01  6.577520e-01 -2.263000e-01  1.984160e-01\n",
      "  -2.349020e-01  3.224690e-01  4.932410e-01 -6.587120e-01  2.295710e-01\n",
      "  -5.544400e-01  1.365350e-01  2.308940e-01  1.010789e+00  4.248180e-01\n",
      "  -5.440570e-01 -3.129000e-01  2.073420e-01  6.814800e-01 -3.537100e-02\n",
      "   6.373520e-01 -2.779060e-01  6.434220e-01  1.769300e-02 -7.975670e-01\n",
      "  -6.809730e-01 -5.032100e-02  4.277949e+00 -5.990410e-01 -7.938300e-02\n",
      "  -4.796450e-01  2.739780e-01  7.508800e-02 -1.251810e-01 -4.308440e-01\n",
      "  -2.337660e-01 -6.911000e-01  2.306090e-01 -3.834000e-03 -7.700370e-01\n",
      "   1.037380e-01  4.398920e-01  4.101030e-01 -8.010140e-01  3.883740e-01\n",
      "  -1.735160e-01 -1.691130e-01  1.941610e-01 -6.557500e-01 -7.672000e-01\n",
      "   3.372030e-01  6.555070e-01 -6.653400e-02  5.986930e-01  1.331900e-01\n",
      "  -7.232200e-02 -4.099440e-01 -2.342830e-01 -3.883000e-03  2.789910e-01\n",
      "   3.747620e-01  1.871670e-01  2.326040e-01 -3.617200e-02 -7.058110e-01\n",
      "  -5.287000e-03 -2.964700e-01  3.212140e-01  1.338240e-01 -7.552490e-01\n",
      "   5.509330e-01  5.371740e-01 -3.284330e-01  5.425760e-01 -1.844940e-01\n",
      "  -1.395680e-01  4.897300e-02 -3.266000e-03  2.477190e-01 -5.672760e-01\n",
      "   0.000000e+00  0.000000e+00]\n",
      " [-2.330880e-01  2.944840e-01 -1.577780e-01 -2.557480e-01 -2.874840e-01\n",
      "   2.465730e-01  2.474580e-01 -3.111340e-01  7.250170e-01  5.148990e-01\n",
      "   2.209220e-01 -9.286290e-01  6.135940e-01  7.221540e-01  7.838130e-01\n",
      "   7.748300e-02  4.651310e-01  8.514800e-02  4.723710e-01  1.736080e-01\n",
      "  -4.588690e-01  2.225980e-01  5.960330e-01 -2.425660e-01 -6.748560e-01\n",
      "  -7.426330e-01  1.263611e+00  5.812400e-02 -7.271600e-01  1.235700e-02\n",
      "  -3.107050e-01 -1.630250e-01  1.540670e-01  5.613770e-01 -8.831900e-02\n",
      "  -4.348110e-01 -7.057800e-01  4.475360e-01  2.609770e-01 -1.573780e-01\n",
      "   5.948680e-01  9.506710e-01 -1.095900e-01  2.319540e-01 -4.853790e-01\n",
      "  -2.840410e-01 -2.560980e-01  3.362880e+00 -1.551280e-01 -1.481570e-01\n",
      "  -9.403400e-02 -8.983700e-02 -2.388050e-01  9.480560e-01  8.483800e-02\n",
      "  -7.377690e-01 -2.604300e-02  8.238390e-01 -8.063020e-01 -6.768610e-01\n",
      "   5.123860e-01 -1.598560e-01  9.825000e-03 -2.675900e-01  1.155220e-01\n",
      "   1.744600e-01 -1.624840e-01 -4.000280e-01 -1.063416e+00  3.912400e-02\n",
      "   7.533960e-01  5.077390e-01 -6.966450e-01  5.301050e-01 -5.993300e-02\n",
      "  -2.159930e-01  3.286520e-01 -1.130270e-01 -1.638267e+00  4.816790e-01\n",
      "   5.757950e-01 -1.422700e-02 -4.737280e-01 -1.306330e-01 -2.207250e-01\n",
      "  -5.822060e-01  5.207170e-01  1.422840e-01  3.666040e-01 -3.090970e-01\n",
      "   3.056280e-01 -1.338390e-01 -8.961750e-01  8.049600e-02 -3.496170e-01\n",
      "  -1.969600e-02 -5.631760e-01  2.080470e-01  3.579880e-01  1.027745e+00\n",
      "   0.000000e+00  0.000000e+00]\n",
      " [ 0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
      "   0.000000e+00  0.000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS = {}\n",
    "\n",
    "with gzip.open('vector_it_100.txt.gz', 'rb') as f:\n",
    "    for vec in (l.split() for l in f):\n",
    "        # note that float32 becomes a pytorch float, but float64 becomes a double!\n",
    "        EMBEDDINGS[vec[0].decode()] = np.array(\n",
    "            [float(x) for x in vec[1:]],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "def encode_from_embeddings(tokens):\n",
    "    \"\"\"Transform tokens into vectors using their embeddings and casing.\n",
    "    The GloVe embedding is used, zeros if not found, and then two values\n",
    "    are used to represent the casing.\n",
    "    \"\"\"\n",
    "    result_array = []\n",
    "    for token in tokens:\n",
    "        if token.lower() not in EMBEDDINGS:\n",
    "            result_array.append(np.zeros(TOKEN_INPUT_SIZE))\n",
    "            continue\n",
    "            \n",
    "        if token.lower() == token:\n",
    "            meta = np.array([0, 0], dtype=np.float32)\n",
    "        elif token.capitalize() == token:\n",
    "            meta = np.array([1, 0], dtype=np.float32)\n",
    "        elif token.upper() == token:\n",
    "            meta = np.array([1, 1], dtype=np.float32)\n",
    "        else:\n",
    "            # unnamed casing\n",
    "            meta = np.array([0, 1], dtype=np.float32)\n",
    "        \n",
    "        vect = np.hstack((EMBEDDINGS[token.lower()], meta))\n",
    "        \n",
    "        result_array.append(vect)\n",
    "    return np.array(result_array, dtype=np.float32)\n",
    "        \n",
    "def encode_as_one_hot(tokens):\n",
    "    return oh_enc_words.transform(np.array(tokens).reshape(-1, 1))\n",
    "\n",
    "\n",
    "test_words = ['il', 'mio', 'fakewordsnotexisting']\n",
    "print(encode_as_one_hot(test_words))\n",
    "print(encode_from_embeddings(test_words))\n",
    "\n",
    "assert encode_as_one_hot(test_words).shape == encode_from_embeddings(test_words).shape\n",
    "\n",
    "ENCODER_FUNCTION = encode_from_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now an utterance can be transformed in a list of fixed-length features and corresponding outputs.\n",
    "\n",
    "To do this, for every token a fixed window of tokens is exported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_TEXT = 'e ora che faccio?'\n",
    "\n",
    "WINDOW_LEFT_SIZE = 7 # words before the punctuation element\n",
    "WINDOW_RIGHT_SIZE = 5 # words after the punctuation element\n",
    "\n",
    "def utterance_to_features_set(utterance: str):\n",
    "    tokens = list(words_punctuation_tuples(utterance))\n",
    "    # fill with 2 just to later check that nothing is left unassigned\n",
    "    X = np.ones((\n",
    "        len(tokens),\n",
    "        TOKEN_INPUT_SIZE * (WINDOW_LEFT_SIZE + WINDOW_RIGHT_SIZE)\n",
    "    ), dtype=np.float32) * 2\n",
    "    \n",
    "    Y = np.ones((\n",
    "        len(tokens),\n",
    "        KNOWN_PUNCTUATIONS), dtype=np.float32)\n",
    "    \n",
    "    words_vectors = ENCODER_FUNCTION([w for w, _ in tokens])\n",
    "    words_vectors = np.vstack((\n",
    "        np.ones((WINDOW_LEFT_SIZE, TOKEN_INPUT_SIZE)),\n",
    "        words_vectors,\n",
    "        np.ones((WINDOW_RIGHT_SIZE, TOKEN_INPUT_SIZE))\n",
    "    ))\n",
    "    \n",
    "    for idx, (word, punct) in enumerate(tokens):\n",
    "        words_window = words_vectors[idx: idx + WINDOW_RIGHT_SIZE + WINDOW_LEFT_SIZE , :]\n",
    "        \n",
    "        X[idx, :] = words_window.reshape(-1)\n",
    "        Y[idx, :] = oh_enc_punct.transform(np.array([punct]).reshape(-1, 1))\n",
    "    return X, Y\n",
    "\n",
    "X, Y = utterance_to_features_set(DEMO_TEXT)\n",
    "# check that the value 2 was not left anywhere\n",
    "assert np.max(X) != 2.0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, I immediately define an helper to generate and show the punctuation from the same Y array, to immediately see the result from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e ora che faccio?', 4, 4)\n"
     ]
    }
   ],
   "source": [
    "X, Y = utterance_to_features_set(DEMO_TEXT)\n",
    "def punct_to_text(utterance: str, Y: np.array) -> (str, int, int):\n",
    "    \"\"\"Return the utterance with the predicted punctuation.\n",
    "    \n",
    "    Also count how many were correct and return the (OK, total) count\n",
    "    \n",
    "    The original punctuation is replaced with the one in the vector Y,\n",
    "    which is one-hot encoded.\n",
    "    \"\"\"\n",
    "    assigned_puncts = oh_enc_punct.inverse_transform(Y)\n",
    "    ret = []\n",
    "    ok = 0\n",
    "    for (w, correct), predicted in zip(words_punctuation_tuples(utterance), assigned_puncts):\n",
    "        ret.append(w)\n",
    "        if predicted[0] is None:\n",
    "            ret.append('[???]')\n",
    "        else:\n",
    "            ret.append(predicted[0])\n",
    "        if predicted[0] == correct:\n",
    "            ok += 1\n",
    "    return ''.join(ret), ok, len(assigned_puncts)\n",
    "\n",
    "print(punct_to_text(DEMO_TEXT, Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to predict some punctuation using a sample text as training and a 2-NN as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training text:\n",
      " Ma non disperiamo.Siamo incoraggiati da quello che abbiamo letto, sentito alla radio, o visto in TV; riguardo a ciò che i nostri fratelli e sorelle del Nord stanno facendo per determinare la forma che dovrà prendere la nuova economia mondiale. I loro modi di lottare sono allo stesso tempo così simili e così diversi dai nostri. Mano a mano che la nostra lotta emerge, impariamo meglio come combattere con più forza chi ci sta facendo del male. Non faremo gli errori del passato, quando troppo spesso abbiamo confidato in leader o partiti o nazione o razze, perché ci portassero alla salvezza. Ora sappiamo che solo la libertà e la giustizia che noi tutti costruiremo assieme ha la forza per resistere alla oppressione.\n",
      "\n",
      "prediction text:\n",
      " che ricompensa avrete?\")\n",
      "---\n",
      "\n",
      "predicted punctuation:\n",
      " ('che ricompensa avrete ', 2, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "\n",
    "print('training text:\\n', utterances[0])\n",
    "X_train, Y_train = utterance_to_features_set(utterances[0])\n",
    "neigh.fit(X_train, Y_train)\n",
    "\n",
    "X_predict, Y_predict = utterance_to_features_set(utterances[2])\n",
    "Y = neigh.predict(X_predict)\n",
    "print('\\nprediction text:\\n', utterances[2])\n",
    "print('---')\n",
    "print('\\npredicted punctuation:\\n', punct_to_text(utterances[2], Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "VALIDATION_UTTERANCES = 3000\n",
    "PARTIAL_FIT_CHUNK_SIZE = 1000\n",
    "\n",
    "def measure_model(model, partial_fit=None):\n",
    "    \"\"\"Train and predict many times and show the accuracy.\n",
    "    \n",
    "    partial_fit enforces an incremental fit, if left None the\n",
    "    behavior is decided based on the attribute availability\n",
    "    \"\"\"\n",
    "    # in case of partial_fit available\n",
    "    if partial_fit is None:\n",
    "        partial_fit = hasattr(model, 'partial_fit')\n",
    "    \n",
    "    if partial_fit:\n",
    "        training_ranges = [\n",
    "            (i, i + PARTIAL_FIT_CHUNK_SIZE) \n",
    "            for i in range(0, len(utterances) -  VALIDATION_UTTERANCES, PARTIAL_FIT_CHUNK_SIZE)\n",
    "        ]\n",
    "    else:\n",
    "        training_ranges = [\n",
    "            (0, i) \n",
    "            for i in range(10, len(utterances) -  VALIDATION_UTTERANCES, PARTIAL_FIT_CHUNK_SIZE)\n",
    "        ]\n",
    "        \n",
    "    for start_idx, end_idx in training_ranges:\n",
    "        X_train = None\n",
    "        Y_train = None\n",
    "        \n",
    "        for ut in utterances[start_idx:end_idx]:\n",
    "            X, Y = utterance_to_features_set(ut)\n",
    "            if X_train is None:\n",
    "                X_train = X\n",
    "                Y_train = Y\n",
    "            else:\n",
    "                X_train = np.vstack((X_train, X))\n",
    "                Y_train = np.vstack((Y_train, Y))\n",
    "        print(f'{datetime.now().isoformat()} Training with range'\n",
    "              f' {(start_idx, end_idx)}, partial fit: {partial_fit}')\n",
    "        if partial_fit:\n",
    "            model.partial_fit(X_train, Y_train)\n",
    "        else:\n",
    "            model.fit(X_train, Y_train)\n",
    "\n",
    "        total_ok = 0\n",
    "        total_in_validation = 0\n",
    "\n",
    "        for ut in utterances[-VALIDATION_UTTERANCES:]:\n",
    "            X, _ = utterance_to_features_set(ut)\n",
    "            Y = model.predict(X)\n",
    "            reconstructed, ok, total = punct_to_text(ut, Y)\n",
    "\n",
    "            total_in_validation += total\n",
    "            total_ok += ok\n",
    "            # print('\\nprediction text:\\n', ut)\n",
    "            # print('\\npredicted punctuation:\\n', reconstructed)\n",
    "\n",
    "        print(f'{datetime.now().isoformat()} - After training until utterance {end_idx}'\n",
    "              f' there were {total_ok}/{total_in_validation} correct values'\n",
    "              f', ({total_ok * 100 / total_in_validation:0.3f}% of the total)')\n",
    "        with open('score.csv', 'a') as f:\n",
    "            f.write(f'{datetime.now().isoformat()}\\t{end_idx}\\t{total_ok * 100 / total_in_validation:0.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-neighbor with 3 neighbors:\n",
      "2019-11-27T18:55:26.846869 Training with range (0, 10), partial fit: False\n",
      "2019-11-27T19:35:12.361828 - After training until utterance 10 there were 124966/153269 correct values, (81.534% of the total)\n",
      "2019-11-27T19:37:05.148533 Training with range (0, 1010), partial fit: False\n",
      "2019-11-27T21:18:47.300638 - After training until utterance 1010 there were 127954/153269 correct values, (83.483% of the total)\n",
      "2019-11-27T21:25:21.134954 Training with range (0, 2010), partial fit: False\n",
      "2019-11-28T09:41:07.470083 - After training until utterance 2010 there were 127969/153269 correct values, (83.493% of the total)\n",
      "2019-11-28T09:58:42.681466 Training with range (0, 3010), partial fit: False\n",
      "2019-11-28T15:28:48.312468 - After training until utterance 3010 there were 128499/153269 correct values, (83.839% of the total)\n",
      "2019-11-28T15:57:30.444239 Training with range (0, 4010), partial fit: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2166144361e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K-neighbor with 3 neighbors:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmeasure_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-0c1f734c2d7a>\u001b[0m in \u001b[0;36mmeasure_model\u001b[0;34m(model, partial_fit)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mut\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutterances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mVALIDATION_UTTERANCES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutterance_to_features_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mreconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunct_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.scientific_jupyter/lib/python3.7/site-packages/sklearn/neighbors/regression.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.scientific_jupyter/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 delayed_query(\n\u001b[1;32m    453\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 454\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             )\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.scientific_jupyter/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.scientific_jupyter/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "print('K-neighbor with 3 neighbors:')\n",
    "measure_model(KNeighborsRegressor(n_neighbors=3, n_jobs=-1))\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "print('Decision tree:')\n",
    "measure_model(DecisionTreeRegressor())\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "print('K-neighbor with 5 neighbors:')\n",
    "measure_model(KNeighborsRegressor(n_neighbors=5, n_jobs=-1))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print('Random Forest with 5 trees:')\n",
    "measure_model(RandomForestClassifier(n_estimators=5, n_jobs=-1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with neural networks using skorch.\n",
    "First we define a network with 2 layers, each with 10 neurons, having tanh as the activation function with relu at the output layer.\n",
    "\n",
    "Then, skorch wraps it and make it possible to use it as a scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network:\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import tanh\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "class RegressorModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=40,\n",
    "            nonlin=tanh,\n",
    "    ):\n",
    "        super(RegressorModule, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(\n",
    "            TOKEN_INPUT_SIZE * (WINDOW_LEFT_SIZE + WINDOW_RIGHT_SIZE),\n",
    "            num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dense1 = nn.Linear(num_units, 40)\n",
    "        self.dense2 = nn.Linear(num_units, 40)\n",
    "        self.output = nn.Linear(40, KNOWN_PUNCTUATIONS)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = F.relu(X)\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "net_regr = NeuralNetRegressor(\n",
    "    RegressorModule,\n",
    "    max_epochs=30,\n",
    "    lr=0.003,\n",
    ")\n",
    "\n",
    "# these two steps are necessary to load the weights\n",
    "net_regr.initialize()\n",
    "net_regr.load_params(f_params='punctuation_weights.pkl')\n",
    "\n",
    "print('Neural network:')\n",
    "measure_model(net_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Warning: mysql_connect() [function.mysql-connect]: Access denied for user 'imc_italy'@'localhost' (using password: YES) in /imc/sf-active/shared/classes/db_class.inc on line 28\n",
      "Reconstructed:\n",
      "Warning: mysql_connect() [function.mysql-connect]: Access denied for user 'imc_italy'@'localhost' (using password: YES) in /imc/sf-active/shared/classes/db_class.inc on line 28\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "Come la cometa ad annunciare una nuova era questo post apparve nella notte\n",
      "Reconstructed:\n",
      "Come la cometa ad annunciare una nuova era questo post apparve nella notte.\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "una volta per tutte il dominio della forza negli affari\n",
      "Reconstructed:\n",
      "una volta per tutte il dominio della forza negli affari.\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "Warning: mysql_fetch_array(): supplied argument is not a valid MySQL result resource in /imc/sf-active/shared/classes/db_class.inc on line 50\n",
      "Reconstructed:\n",
      "Warning: mysql_fetch_array(): supplied argument is not a valid MySQL result resource in /imc/sf-active/shared/classes/db_class.inc on line 50\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "Axel Springer (1)\n",
      "Reconstructed:\n",
      "Axel Springer (1)\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "> DATE: Mon, 13 Oct 2003 17:49:23\n",
      "Reconstructed:\n",
      " DATE Mon 13 Oct 2003 17 49 23\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "né ci appartengono i chiarimenti con le virgolette.\n",
      "Reconstructed:\n",
      "né ci appartengono i chiarimenti con le virgolette.\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "Ci sono un sacco di lingue con un alfabeto riccamente diverso dal quello occidentale. Anzi per la verità questo riguarda la maggioranza della popolazione mondiale. Ma le tastiere che ne tengono conto non sono sempre facili da trovare.\n",
      "Reconstructed:\n",
      "Ci sono un sacco di lingue con un alfabeto riccamente diverso dal quello occidentale. Anzi per la verità questo riguarda la maggioranza della popolazione mondiale. Ma le tastiere che ne tengono conto non sono sempre facili da trovare.\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "La missione del Columbia rimarrà nella storia per tante ragioni, ma una è sconosciuta alla maggior parte dell’umanità. Una brutale entità proveniente dallo spazio esterno approfitta dello Space Shuttle per raggiungere il nostro mondo e usarne gli abitanti come incubatrici viventi per generare nuovi membri della propria specie, veri e propri abominii genetici, da mpiegare in una guerra senza fine. E la razza umana non ha alcun mezzo per contrastare – o anche solo comprentere – la mostruosa e cieca violenza di questa orripilante invasione dall’interno…\n",
      "Reconstructed:\n",
      "La missione del Columbia rimarrà nella storia per tante ragioni ma una è sconosciuta alla maggior parte dell’umanità. Una brutale entità proveniente dallo spazio esterno approfitta dello Space Shuttle per raggiungere il nostro mondo e usarne gli abitanti come incubatrici viventi per generare nuovi membri della propria specie veri e propri abominii genetici da mpiegare in una guerra senza fine. E la razza umana non ha alcun mezzo per contrastare o anche solo comprentere la mostruosa e cieca violenza di questa orripilante invasione dall’interno.\n",
      "\n",
      "---\n",
      "\n",
      "Original:\n",
      "IL APRTITO COMUNISTA CINESE MI HA INDOTTO HA SPINGERNI VERSO L'ANARKIA ASSIEME A SVARIATE LETTURE DEL' GRANDE BAKUNIN OVVIAMENTE, in quanto prima quando ero solo un ragazzino mi sentivo di esere un comunista , poi sono cambiato in meglio nel ' tempo, ( ancora oggi rieso a essere daccondo sù alcuni punti di vista sul comunismo o sù alcune idee e proposte avanzate da comunisti inseriti nel movimento dei movimenti.)..........fioi stò skrivendo alla kazzo non rispetto la punteggiatura la grammatica l'orto grafia e non menefraga un kazzo : )\n",
      "Reconstructed:\n",
      "IL APRTITO COMUNISTA CINESE MI HA INDOTTO HA SPINGERNI VERSO L’ANARKIA ASSIEME A SVARIATE LETTURE DEL GRANDE BAKUNIN OVVIAMENTE in quanto prima quando ero solo un ragazzino mi sentivo di esere un comunista poi sono cambiato in meglio nel tempo ancora oggi rieso a essere daccondo sù alcuni punti di vista sul comunismo o sù alcune idee e proposte avanzate da comunisti inseriti nel movimento dei movimenti fioi stò skrivendo alla kazzo non rispetto la punteggiatura la grammatica l’orto grafia e non menefraga un kazzo.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "for _ in range(10):\n",
    "    ut = choice(utterances)\n",
    "    print('Original:')\n",
    "    print(ut)\n",
    "    # remove most of the punctuation to ensure that there's no\n",
    "    # side channel or bug allowing the model to \"cheat\"\n",
    "    ut =  ut.replace('.', '-')\n",
    "    ut =  ut.replace(',', '-')\n",
    "    ut =  ut.replace(';', '-')\n",
    "    ut =  ut.replace('’', '-')\n",
    "    ut =  ut.replace('?', '-')\n",
    "    ut =  ut.replace('!', '-')\n",
    "    \n",
    "    X, _ = utterance_to_features_set(ut)\n",
    "    Y = net_regr.predict(X)\n",
    "    reconstructed, ok, total = punct_to_text(ut, Y)\n",
    "    print('Reconstructed:')\n",
    "    print(reconstructed)\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_regr.save_params(f_params='punctuation_weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey amico mi dici qual è il problema.\n"
     ]
    }
   ],
   "source": [
    "ut = \"hey amico mi dici qual'è il problema?\"\n",
    "X, _ = utterance_to_features_set(ut)\n",
    "Y = net_reg\n",
    "r.predict(X)\n",
    "reconstructed, ok, total = punct_to_text(ut, Y)\n",
    "print(reconstructed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
