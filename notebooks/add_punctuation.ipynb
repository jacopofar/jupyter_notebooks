{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add punctuation to a text\n",
    "\n",
    "In this notebook we'll try to build a small neural network to apply punctuation to a text, using Skorch to integrate Pytorch with Scikit-learn and be able to easily compare the performance with other ML tools for this use case.\n",
    "\n",
    "For simplicity, the network will be quite small and fast to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gzip\n",
    "from random import shuffle\n",
    "\n",
    "MAX_UTTERANCES_TO_LOAD = 400_000\n",
    "\n",
    "utterances = []\n",
    "\n",
    "with gzip.open('paisa.raw.utf8.gz', 'rb') as f:\n",
    "    for line in (l.decode() for l in f):\n",
    "        if len(line.strip()) < 15:\n",
    "            continue\n",
    "        if line.startswith('<text') or line.startswith('</text>'):\n",
    "            continue\n",
    "        utterances.append(line.strip())\n",
    "        if len(utterances) >= MAX_UTTERANCES_TO_LOAD:\n",
    "            break\n",
    "\n",
    "shuffle(utterances)\n",
    "\n",
    "# rough number of tokens\n",
    "print('Utterances:', len(utterances))\n",
    "print('Longest one (chars):', max(len(u) for u in utterances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function return tuples of text and punctuation, using `isalpha()` and `isdigit()` to tell them apart and accept multiple characters for the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_punctuation_tuples(text: str):\n",
    "    \"\"\"Return tuples of text and subsequent punctuation from a text.\"\"\"\n",
    "    am_in_token = True\n",
    "    token_start = 0\n",
    "    punctuation_start = 0\n",
    "    \n",
    "    for idx, char in enumerate(text):\n",
    "        if am_in_token:\n",
    "            if char.isalpha() or char.isdigit():\n",
    "                # just a new char for this token\n",
    "                pass\n",
    "            else:\n",
    "                # switch to punctuation\n",
    "                am_in_token = False\n",
    "                punctuation_start = idx\n",
    "        else:\n",
    "            if char.isalpha() or char.isdigit():\n",
    "                # a new token, the punctuation is over\n",
    "                am_in_token = True\n",
    "                yield (\n",
    "                    text[token_start:punctuation_start],\n",
    "                    text[punctuation_start:idx])\n",
    "                token_start = idx\n",
    "            else:\n",
    "                # just a new char for the punctuation\n",
    "                pass\n",
    "    \n",
    "    if am_in_token:\n",
    "        # the last text has an empty token associated\n",
    "        yield (text[token_start:], '')\n",
    "    else:\n",
    "        # the punctuation was the end\n",
    "        yield (text[token_start:punctuation_start], text[punctuation_start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, p in words_punctuation_tuples(utterances[0]):\n",
    "    print(f\"'{w}', '{p}'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# possible punctuations used for hot encoding, ignore the others\n",
    "KNOWN_PUNCTUATIONS = 40\n",
    "\n",
    "punctuation = Counter([punct for ut in utterances for _, punct in words_punctuation_tuples(ut)])\n",
    "total_tokens = sum(punctuation.values())\n",
    "print(f'Total tokens: {total_tokens}')\n",
    "print(f'Will use the {KNOWN_PUNCTUATIONS} most common, which are:')\n",
    "\n",
    "total_partial = 0\n",
    "punct_categories = []\n",
    "for punct, count in punctuation.most_common(KNOWN_PUNCTUATIONS):\n",
    "    punct_categories.append(punct)\n",
    "    punct = punct.replace('\\n', '\\\\n')\n",
    "    total_partial += count\n",
    "    print(f\"Symbol \\t'{punct}'\\t appears {count} \\t times\"\n",
    "          f' ({count * 100 / total_tokens:0.2f}%)')\n",
    "print('---')\n",
    "print(f'Eventually covering {total_partial} out of {total_tokens}'\n",
    "      f' ({total_partial * 100 / total_tokens:0.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequencies, uncased\n",
    "words_freq = Counter([word for ut in utterances for word, _ in words_punctuation_tuples(ut)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens for the hot encoding, ignore the others\n",
    "# or if using embeddings, the size of the embedding + casing encoding\n",
    "TOKEN_INPUT_SIZE = 100 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "token_categories = [w for w, _ in words_freq.most_common(TOKEN_INPUT_SIZE)]\n",
    "total_covered = sum(c for _, c in words_freq.most_common(TOKEN_INPUT_SIZE))\n",
    "print(f'Most common five words: {words_freq.most_common(5)}')\n",
    "print(f'With {TOKEN_INPUT_SIZE} we cover {total_covered} tokens out of {total_tokens}'\n",
    "      f', ({total_covered * 100 / total_tokens:0.2f}% of the total)')\n",
    "\n",
    "# map them to non-sparse one-hot encoding\n",
    "oh_enc_words = OneHotEncoder(\n",
    "    sparse=False, # use an array\n",
    "    handle_unknown='ignore', # return all 0 in case of unknown string, no error\n",
    "    categories='auto', # enumerate categories later, during training\n",
    "    dtype=np.float32, # use float32 or pytorch will convert to double\n",
    ")\n",
    "\n",
    "oh_enc_words.fit_transform(np.array(token_categories).reshape(-1, 1))\n",
    "\n",
    "# show that it works both ways\n",
    "tokens = np.array(['il', 'che']).reshape(-1, 1)\n",
    "encoded = oh_enc_words.transform(tokens)\n",
    "decoded = oh_enc_words.inverse_transform(encoded)\n",
    "print(f'Original: {tokens}')\n",
    "print(f'Reconstructed: {decoded}')\n",
    "assert np.array_equal(decoded, tokens)\n",
    "\n",
    "# show that it ignores unknown tokens\n",
    "tokens = np.array(['fakewordnotexisting']).reshape(-1, 1)\n",
    "encoded = oh_enc_words.transform(tokens)\n",
    "assert np.all(encoded == 0)\n",
    "\n",
    "del encoded, decoded, tokens\n",
    "\n",
    "# same with punctuation\n",
    "\n",
    "oh_enc_punct = OneHotEncoder(\n",
    "    sparse=False,\n",
    "    handle_unknown='ignore',\n",
    "    categories='auto' # enumerate categories later, during training\n",
    ")\n",
    "oh_enc_punct.fit_transform(np.array(punct_categories).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to encode words this way, but then let's encode using word embeddings. I use my own GloVe data for Italian, cased, available here: https://github.com/jacopofar/glove-tools/releases\n",
    "\n",
    "Still the function based on embeddings is available and can be used by changing the `ENCODER_FUNCTION` variable accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS = {}\n",
    "\n",
    "with gzip.open('vector_it_100.txt.gz', 'rb') as f:\n",
    "    for vec in (l.split() for l in f):\n",
    "        # note that float32 becomes a pytorch float, but float64 becomes a double!\n",
    "        EMBEDDINGS[vec[0].decode()] = np.array(\n",
    "            [float(x) for x in vec[1:]],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "def encode_from_embeddings(tokens):\n",
    "    \"\"\"Transform tokens into vectors using their embeddings and casing.\n",
    "    The GloVe embedding is used, zeros if not found, and then two values\n",
    "    are used to represent the casing.\n",
    "    \"\"\"\n",
    "    result_array = []\n",
    "    for token in tokens:\n",
    "        if token.lower() not in EMBEDDINGS:\n",
    "            result_array.append(np.zeros(TOKEN_INPUT_SIZE))\n",
    "            continue\n",
    "            \n",
    "        if token.lower() == token:\n",
    "            meta = np.array([0, 0], dtype=np.float32)\n",
    "        elif token.capitalize() == token:\n",
    "            meta = np.array([1, 0], dtype=np.float32)\n",
    "        elif token.upper() == token:\n",
    "            meta = np.array([1, 1], dtype=np.float32)\n",
    "        else:\n",
    "            # unnamed casing\n",
    "            meta = np.array([0, 1], dtype=np.float32)\n",
    "        \n",
    "        vect = np.hstack((EMBEDDINGS[token.lower()], meta))\n",
    "        \n",
    "        result_array.append(vect)\n",
    "    return np.array(result_array, dtype=np.float32)\n",
    "        \n",
    "def encode_as_one_hot(tokens):\n",
    "    return oh_enc_words.transform(np.array(tokens).reshape(-1, 1))\n",
    "\n",
    "\n",
    "test_words = ['il', 'mio', 'fakewordsnotexisting']\n",
    "print(encode_as_one_hot(test_words))\n",
    "print(encode_from_embeddings(test_words))\n",
    "\n",
    "assert encode_as_one_hot(test_words).shape == encode_from_embeddings(test_words).shape\n",
    "\n",
    "ENCODER_FUNCTION = encode_from_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now an utterance can be transformed in a list of fixed-length features and corresponding outputs.\n",
    "\n",
    "To do this, for every token a fixed window of tokens is exported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_TEXT = 'e ora che faccio?'\n",
    "\n",
    "WINDOW_LEFT_SIZE = 7 # words before the punctuation element\n",
    "WINDOW_RIGHT_SIZE = 5 # words after the punctuation element\n",
    "\n",
    "def utterance_to_features_set(utterance: str):\n",
    "    tokens = list(words_punctuation_tuples(utterance))\n",
    "    # fill with 2 just to later check that nothing is left unassigned\n",
    "    X = np.ones((\n",
    "        len(tokens),\n",
    "        TOKEN_INPUT_SIZE * (WINDOW_LEFT_SIZE + WINDOW_RIGHT_SIZE)\n",
    "    ), dtype=np.float32) * 2\n",
    "    \n",
    "    Y = np.ones((\n",
    "        len(tokens),\n",
    "        KNOWN_PUNCTUATIONS), dtype=np.float32)\n",
    "    \n",
    "    words_vectors = ENCODER_FUNCTION([w for w, _ in tokens])\n",
    "    words_vectors = np.vstack((\n",
    "        np.ones((WINDOW_LEFT_SIZE, TOKEN_INPUT_SIZE)),\n",
    "        words_vectors,\n",
    "        np.ones((WINDOW_RIGHT_SIZE, TOKEN_INPUT_SIZE))\n",
    "    ))\n",
    "    \n",
    "    for idx, (word, punct) in enumerate(tokens):\n",
    "        words_window = words_vectors[idx: idx + WINDOW_RIGHT_SIZE + WINDOW_LEFT_SIZE , :]\n",
    "        \n",
    "        X[idx, :] = words_window.reshape(-1)\n",
    "        Y[idx, :] = oh_enc_punct.transform(np.array([punct]).reshape(-1, 1))\n",
    "    return X, Y\n",
    "\n",
    "X, Y = utterance_to_features_set(DEMO_TEXT)\n",
    "# check that the value 2 was not left anywhere\n",
    "assert np.max(X) != 2.0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, I immediately define an helper to generate and show the punctuation from the same Y array, to immediately see the result from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = utterance_to_features_set(DEMO_TEXT)\n",
    "def punct_to_text(utterance: str, Y: np.array) -> (str, int, int):\n",
    "    \"\"\"Return the utterance with the predicted punctuation.\n",
    "    \n",
    "    Also count how many were correct and return the (OK, total) count\n",
    "    \n",
    "    The original punctuation is replaced with the one in the vector Y,\n",
    "    which is one-hot encoded.\n",
    "    \"\"\"\n",
    "    assigned_puncts = oh_enc_punct.inverse_transform(Y)\n",
    "    ret = []\n",
    "    ok = 0\n",
    "    for (w, correct), predicted in zip(words_punctuation_tuples(utterance), assigned_puncts):\n",
    "        ret.append(w)\n",
    "        if predicted[0] is None:\n",
    "            ret.append('[???]')\n",
    "        else:\n",
    "            ret.append(predicted[0])\n",
    "        if predicted[0] == correct:\n",
    "            ok += 1\n",
    "    return ''.join(ret), ok, len(assigned_puncts)\n",
    "\n",
    "print(punct_to_text(DEMO_TEXT, Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to predict some punctuation using a sample text as training and a 2-NN as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "\n",
    "print('training text:\\n', utterances[0])\n",
    "X_train, Y_train = utterance_to_features_set(utterances[0])\n",
    "neigh.fit(X_train, Y_train)\n",
    "\n",
    "X_predict, Y_predict = utterance_to_features_set(utterances[2])\n",
    "Y = neigh.predict(X_predict)\n",
    "print('\\nprediction text:\\n', utterances[2])\n",
    "print('---')\n",
    "print('\\npredicted punctuation:\\n', punct_to_text(utterances[2], Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "VALIDATION_UTTERANCES = 3000\n",
    "PARTIAL_FIT_CHUNK_SIZE = 1000\n",
    "\n",
    "def measure_model(model, partial_fit=None):\n",
    "    \"\"\"Train and predict many times and show the accuracy.\n",
    "    \n",
    "    partial_fit enforces an incremental fit, if left None the\n",
    "    behavior is decided based on the attribute availability\n",
    "    \"\"\"\n",
    "    # in case of partial_fit available\n",
    "    if partial_fit is None:\n",
    "        partial_fit = hasattr(model, 'partial_fit')\n",
    "    \n",
    "    if partial_fit:\n",
    "        training_ranges = [\n",
    "            (i, i + PARTIAL_FIT_CHUNK_SIZE) \n",
    "            for i in range(0, len(utterances) -  VALIDATION_UTTERANCES, PARTIAL_FIT_CHUNK_SIZE)\n",
    "        ]\n",
    "    else:\n",
    "        training_ranges = [\n",
    "            (0, i) \n",
    "            for i in range(10, len(utterances) -  VALIDATION_UTTERANCES, PARTIAL_FIT_CHUNK_SIZE)\n",
    "        ]\n",
    "        \n",
    "    for start_idx, end_idx in training_ranges:\n",
    "        X_train = None\n",
    "        Y_train = None\n",
    "        \n",
    "        for ut in utterances[start_idx:end_idx]:\n",
    "            X, Y = utterance_to_features_set(ut)\n",
    "            if X_train is None:\n",
    "                X_train = X\n",
    "                Y_train = Y\n",
    "            else:\n",
    "                X_train = np.vstack((X_train, X))\n",
    "                Y_train = np.vstack((Y_train, Y))\n",
    "        print(f'{datetime.now().isoformat()} Training with range'\n",
    "              f' {(start_idx, end_idx)}, partial fit: {partial_fit}')\n",
    "        if partial_fit:\n",
    "            model.partial_fit(X_train, Y_train)\n",
    "        else:\n",
    "            model.fit(X_train, Y_train)\n",
    "\n",
    "        total_ok = 0\n",
    "        total_in_validation = 0\n",
    "\n",
    "        for ut in utterances[-VALIDATION_UTTERANCES:]:\n",
    "            X, _ = utterance_to_features_set(ut)\n",
    "            Y = model.predict(X)\n",
    "            reconstructed, ok, total = punct_to_text(ut, Y)\n",
    "\n",
    "            total_in_validation += total\n",
    "            total_ok += ok\n",
    "            # print('\\nprediction text:\\n', ut)\n",
    "            # print('\\npredicted punctuation:\\n', reconstructed)\n",
    "\n",
    "        print(f'{datetime.now().isoformat()} - After training until utterance {end_idx}'\n",
    "              f' there were {total_ok}/{total_in_validation} correct values'\n",
    "              f', ({total_ok * 100 / total_in_validation:0.3f}% of the total)')\n",
    "        with open('score.csv', 'a') as f:\n",
    "            f.write(f'{datetime.now().isoformat()}\\t{end_idx}\\t{total_ok * 100 / total_in_validation:0.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "print('K-neighbor with 3 neighbors:')\n",
    "measure_model(KNeighborsRegressor(n_neighbors=3, n_jobs=-1))\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "print('Decision tree:')\n",
    "measure_model(DecisionTreeRegressor())\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "print('K-neighbor with 5 neighbors:')\n",
    "measure_model(KNeighborsRegressor(n_neighbors=5, n_jobs=-1))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print('Random Forest with 5 trees:')\n",
    "measure_model(RandomForestClassifier(n_estimators=5, n_jobs=-1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with neural networks using skorch.\n",
    "First we define a network with 2 layers, each with 10 neurons, having tanh as the activation function with relu at the output layer.\n",
    "\n",
    "Then, skorch wraps it and make it possible to use it as a scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import tanh\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "class RegressorModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=40,\n",
    "            nonlin=tanh,\n",
    "    ):\n",
    "        super(RegressorModule, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(\n",
    "            TOKEN_INPUT_SIZE * (WINDOW_LEFT_SIZE + WINDOW_RIGHT_SIZE),\n",
    "            num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dense1 = nn.Linear(num_units, 40)\n",
    "        self.dense2 = nn.Linear(num_units, 40)\n",
    "        self.output = nn.Linear(40, KNOWN_PUNCTUATIONS)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = F.relu(X)\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "net_regr = NeuralNetRegressor(\n",
    "    RegressorModule,\n",
    "    max_epochs=30,\n",
    "    lr=0.003,\n",
    ")\n",
    "\n",
    "# these two steps are necessary to load the weights\n",
    "net_regr.initialize()\n",
    "net_regr.load_params(f_params='punctuation_weights.pkl')\n",
    "\n",
    "print('Neural network:')\n",
    "measure_model(net_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "for _ in range(10):\n",
    "    ut = choice(utterances)\n",
    "    print('Original:')\n",
    "    print(ut)\n",
    "    # remove most of the punctuation to ensure that there's no\n",
    "    # side channel or bug allowing the model to \"cheat\"\n",
    "    ut =  ut.replace('.', '-')\n",
    "    ut =  ut.replace(',', '-')\n",
    "    ut =  ut.replace(';', '-')\n",
    "    ut =  ut.replace('’', '-')\n",
    "    ut =  ut.replace('?', '-')\n",
    "    ut =  ut.replace('!', '-')\n",
    "    \n",
    "    X, _ = utterance_to_features_set(ut)\n",
    "    Y = net_regr.predict(X)\n",
    "    reconstructed, ok, total = punct_to_text(ut, Y)\n",
    "    print('Reconstructed:')\n",
    "    print(reconstructed)\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_regr.save_params(f_params='punctuation_weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut = \"hey amico mi dici qual'è il problema?\"\n",
    "X, _ = utterance_to_features_set(ut)\n",
    "Y = net_reg\n",
    "r.predict(X)\n",
    "reconstructed, ok, total = punct_to_text(ut, Y)\n",
    "print(reconstructed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
